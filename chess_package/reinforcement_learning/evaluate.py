import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import load_results
from stable_baselines3.common.results_plotter import ts2xy
from stable_baselines3.common.monitor import Monitor

from env import CustomChessEnv


def evaluate(time_steps: int):
    # Reload the saved model
    model = PPO.load(f"chess_model_{time_steps}")
    env = CustomChessEnv()
    env = Monitor(env, filename="./tensorboard_logs/")

    # Evaluate with deterministic policy
    mean_reward, std_reward = evaluate_policy(
        model, env, n_eval_episodes=10, render=False
    )
    print(f"Mean Reward: {mean_reward:.2f} Â± {std_reward:.2f}")


def plot_rewards(log_dir):
    data = load_results(log_dir)
    x, y = ts2xy(data, "timesteps")
    plt.plot(x, y)
    plt.xlabel("Timesteps")
    plt.ylabel("Rewards")
    plt.title("Training Reward Progress")
    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    plot_rewards("./tensorboard_logs")
    # evaluate(100000)
